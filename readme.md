 markdown
# ğŸ•¸ï¸ Advanced Web Scraper & Article Dashboard ğŸ“°âœ¨

![Project Banner](https://img.shields.io/badge/Web_Scraper-ğŸ”¥%20Awesome-brightgreen)
![Python](https://img.shields.io/badge/Python-3.9-blue)
![Flask](https://img.shields.io/badge/Flask-1.1.2-yellow)
![Docker](https://img.shields.io/badge/Docker-19.03.12-blueviolet)
![CI/CD](https://img.shields.io/badge/CI%2FCD-GitHub%20Actions-ff69b4)

## ğŸ“œ Project Overview

Welcome to the **Advanced Web Scraper & Article Dashboard**! This project is a comprehensive web scraping tool combined with a dynamic dashboard, built to efficiently gather, analyze, and display articles from multiple websites. Designed with scalability and user experience in mind, it integrates modern technologies to deliver real-time updates, insightful analytics, and seamless user interactions.

## ğŸš€ Key Features

- **ğŸ” Asynchronous Web Scraping:** Efficiently scrape multiple websites concurrently using `asyncio` and `aiohttp`.
- **ğŸ“ OpenAI Integration:** Automatically generate concise summaries of articles with OpenAI's GPT-4.
- **ğŸ“Š Sentiment Analysis:** Assess the sentiment of each article summary using `TextBlob`.
- **ğŸ›¡ï¸ User Authentication:** Secure user registration and login with `Flask-Login` and `Flask-Bcrypt`.
- **ğŸŒ Real-time Updates:** Receive live updates on new articles via WebSockets with `Flask-SocketIO`.
- **ğŸ“ˆ Advanced Analytics:** Visualize data trends and insights with `Matplotlib` and `Seaborn`.
- **ğŸ“§ Email Notifications:** Get notified about new articles through automated email alerts.
- **ğŸ“¦ Dockerized Deployment:** Easily deploy the application using Docker and Docker Compose.
- **ğŸ”„ Scheduler:** Automated scraping tasks scheduled with `APScheduler`.
- **ğŸ› ï¸ Comprehensive Testing:** Ensure reliability with unit and integration tests using `unittest` and `pytest`.
- **ğŸ”’ API Security:** Protect API endpoints with rate limiting and JWT-based authentication.
- **ğŸ“‚ Data Export:** Export scraped data in CSV and PDF formats for offline access.

## ğŸ› ï¸ Technologies Used

- **Programming Language:** Python 3.9 ğŸ
- **Web Framework:** Flask ğŸ•¸ï¸
- **Web Scraping:** Selenium, BeautifulSoup, aiohttp ğŸš€
- **Asynchronous Programming:** asyncio, aiohttp ğŸ’¨
- **Database:** SQLite ğŸ“š
- **Real-time Communication:** Flask-SocketIO ğŸŒ
- **Data Visualization:** Matplotlib, Seaborn ğŸ“Š
- **Containerization:** Docker, Docker Compose ğŸ³
- **CI/CD:** GitHub Actions ğŸ”„
- **Authentication:** Flask-Login, Flask-Bcrypt ğŸ”
- **Email Handling:** Flask-Mail ğŸ“§
- **Error Tracking:** Sentry ğŸ› ï¸
- **Testing:** unittest, pytest ğŸ§ª

## ğŸ“ Project Structure

project/
â”œâ”€â”€ scraper.py
â”œâ”€â”€ models.py
â”œâ”€â”€ config.ini
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ Dockerfile
â”œâ”€â”€ docker-compose.yml
â”œâ”€â”€ .env
â”œâ”€â”€ test_scraper.py
â”œâ”€â”€ tests/
â”‚   â””â”€â”€ test_scraper.py
â”œâ”€â”€ .github/
â”‚   â””â”€â”€ workflows/
â”‚       â””â”€â”€ python-app.yml
â”œâ”€â”€ templates/
â”‚   â”œâ”€â”€ index.html
â”‚   â”œâ”€â”€ login.html
â”‚   â”œâ”€â”€ register.html
â”‚   â””â”€â”€ analytics.html
â”œâ”€â”€ sites.txt
â”œâ”€â”€ keywords.txt
â”œâ”€â”€ proxies.txt
â”œâ”€â”€ existingarticles.txt
â””â”€â”€ hyperlinks.txt

## ğŸ› ï¸ Installation & Setup

### Prerequisites

- **Docker:** Ensure Docker is installed on your machine. [Install Docker](https://docs.docker.com/get-docker/)
- **Git:** Clone the repository using Git. [Install Git](https://git-scm.com/downloads)

### Clone the Repository

git clone https://github.com/yourusername/advanced-web-scraper.git
cd advanced-web-scraper

### Configuration

1. **Create a `.env` File:**

   Create a `.env` file in the root directory and add your environment variables:

    env
   OPENAI_API_KEY=your_openai_api_key
   MAIL_SERVER=smtp.example.com
   MAIL_PORT=587
   MAIL_USERNAME=your_email@example.com
   MAIL_PASSWORD=your_email_password
   MAIL_USE_TLS=True
   MAIL_USE_SSL=False
   SENTRY_DSN=your_sentry_dsn
    

2. **Update `config.ini`:**

   Ensure all configurations in `config.ini` are correctly set, including paths to `geckodriver`, Firefox binary, and proxies.

### Docker Setup

1. **Build the Docker Image:**

    bash
   docker-compose build
    

2. **Run the Docker Containers:**

    bash
   docker-compose up
    

   The application will be accessible at `http://localhost:5000`.

### Local Setup (Without Docker)

1. **Install Dependencies:**

    bash
   pip install -r requirements.txt
   python -m textblob.download_corpora
    

2. **Run the Application:**

    bash
   python scraper.py
    

   Access the dashboard at `http://localhost:5000`.

## ğŸ–¥ï¸ Usage

1. **Register an Account:**

   - Navigate to `http://localhost:5000/register` to create a new account.

2. **Login:**

   - Go to `http://localhost:5000/login` to access your dashboard.

3. **View Scraped Articles:**

   - The main dashboard displays all scraped articles with real-time updates.

4. **Analytics:**

   - Visit `http://localhost:5000/analytics` to view sentiment distributions and article counts per site.

5. **Export Data:**

   - Use the export buttons on the dashboard to download articles as CSV or PDF.

## ğŸ“ˆ Analytics & Insights

Gain valuable insights from the scraped data:

- **Sentiment Analysis:** Understand the overall sentiment of articles.
- **Article Distribution:** See how many articles are scraped from each site.
- **Real-time Updates:** Stay informed with live article additions.

## ğŸ“§ Email Notifications

Stay updated by receiving email alerts whenever new articles matching your interests are scraped.

## ğŸ”„ Scheduler

Automate scraping tasks to run at regular intervals (e.g., every 6 hours) using `APScheduler`.

## ğŸ§ª Testing

Ensure the reliability and integrity of the application with comprehensive tests.

- **Run Tests:**

   bash
  pytest
   

## ğŸ›¡ï¸ Security

Protect your data and API endpoints with robust security measures:

- **User Authentication:** Secure login and registration processes.
- **Rate Limiting:** Prevent abuse of API endpoints.
- **JWT Authentication:** Optional token-based security for APIs.

## ğŸ³ Docker & Deployment

Easily deploy the application using Docker Compose, which manages all necessary services and dependencies.

- **Build and Run with Docker Compose:**

   bash
  docker-compose up --build
   

## ğŸ“Š Data Visualization

Visualize key metrics and trends with intuitive charts and graphs, enhancing your data analysis capabilities.

## ğŸ Error Tracking

Monitor and resolve issues efficiently with integrated error tracking via Sentry.

## ğŸ’Œ Contact & Support

For any questions, suggestions, or support, feel free to reach out:

- **Email:** alexnite728@gmail.com
- **GitHub Issues:** [Open an Issue](https://github.com/yourusername/advanced-web-scraper/issues)

## ğŸ“„ License

This project is licensed under the [MIT License](LICENSE).

---

ğŸ” **Explore, Analyze, and Stay Informed with the Advanced Web Scraper & Article Dashboard!**

 